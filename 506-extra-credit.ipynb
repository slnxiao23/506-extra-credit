{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Vectorized Haversine Distance Calculation using Numpy.\"\"\"\n",
    "    R = 6371  # Radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    delta_lat = lat2 - lat1\n",
    "    delta_lon = lon2 - lon1\n",
    "    a = np.sin(delta_lat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def generate_time_attributes(data):\n",
    "    \"\"\"Generate cyclic time features.\"\"\"\n",
    "    data['timestamp'] = pd.to_datetime(data['trans_date'] + ' ' + data['trans_time'])\n",
    "    data['hour'] = data['timestamp'].dt.hour\n",
    "    data['weekday'] = data['timestamp'].dt.dayofweek\n",
    "    data['hour_sine'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cosine'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    return data\n",
    "\n",
    "def compute_distance_feature(data):\n",
    "    \"\"\"Add distance column using vectorized Haversine calculation.\"\"\"\n",
    "    data['distance_km'] = calculate_haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
    "    return data\n",
    "\n",
    "def robust_label_encoding(train_set, test_set):\n",
    "    \"\"\"Encode all object-type features in both datasets.\"\"\"\n",
    "    label_enc = LabelEncoder()\n",
    "    for col in train_set.select_dtypes(include='object').columns:\n",
    "        train_set[col] = train_set[col].astype(str)\n",
    "        test_set[col] = test_set[col].astype(str)\n",
    "        label_enc.fit(list(train_set[col]) + list(test_set[col]))\n",
    "        train_set[col] = label_enc.transform(train_set[col])\n",
    "        test_set[col] = label_enc.transform(test_set[col])\n",
    "    return train_set, test_set\n",
    "\n",
    "print(\"Step 1: Loading Data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Step 2: Feature Engineering...\")\n",
    "train_df = generate_time_attributes(train_df)\n",
    "test_df = generate_time_attributes(test_df)\n",
    "\n",
    "train_df = compute_distance_feature(train_df)\n",
    "test_df = compute_distance_feature(test_df)\n",
    "\n",
    "# Robust Label Encoding for categorical features\n",
    "train_df, test_df = robust_label_encoding(train_df, test_df)\n",
    "\n",
    "# Drop irrelevant features\n",
    "drop_columns = ['trans_num', 'trans_date', 'trans_time', 'timestamp', 'id']\n",
    "train_df = train_df.drop(columns=drop_columns, errors='ignore')\n",
    "test_ids = test_df['id']\n",
    "test_df = test_df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "# Define features and target\n",
    "X = train_df.drop(columns=['is_fraud'], errors='ignore')\n",
    "y = train_df['is_fraud']\n",
    "X_test = test_df[X.columns]  # Ensure test columns align with train\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)\n",
    "\n",
    "print(\"Step 3: Setting up Cross-Validation...\")\n",
    "n_folds = 5\n",
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "lgb_params = {'n_estimators': 500, 'learning_rate': 0.03, 'num_leaves': 31, 'random_state': 42, 'class_weight': 'balanced'}\n",
    "\n",
    "# Cross-validation training\n",
    "out_of_fold_preds = np.zeros(len(X))\n",
    "test_predictions = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='f1')\n",
    "    \n",
    "    out_of_fold_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_predictions.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"Step 4: Finding Best Threshold...\")\n",
    "best_threshold, best_f1 = 0.5, 0\n",
    "for threshold in np.linspace(0.01, 0.99, 100):\n",
    "    preds = (out_of_fold_preds >= threshold).astype(int)\n",
    "    score = f1_score(y, preds)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"Step 5: Generating Final Predictions...\")\n",
    "final_test_preds = (np.mean(test_predictions, axis=0) >= best_threshold).astype(int)\n",
    "\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({'id': test_ids, 'is_fraud': final_test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Best Threshold from CV: {best_threshold:.3f}, Best OOF F1-score: {best_f1:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
